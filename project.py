# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xvF4eZ5CO-VSLq3YIZvHAqp216PDoqGm
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_fscore_support

#Reading the dataset voice.csv

data = pd.read_csv("voiceDataSet.csv")
data.head()

#To check the distribution of male and female across all the attributes

data.groupby("label").count()

#Encoding the label column. Female to 0 and male to 1

class_mapping = {label: idx for idx, label in enumerate(np.unique(data['label']))}
class_mapping

# Converting class labels from strings to integers

data['label'] = data['label'].map(class_mapping)

#Creating X,y and splitting the dataset into training and testing
import array as arr
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# import warnings filter
from warnings import simplefilter
# ignore all future warnings
simplefilter(action='ignore', category=FutureWarning)
X, y = data.iloc[:, :-1].values, data.iloc[:, -1].values
maxm=0
a=arr.array('f',[])
b=arr.array('i',[])
for i in range (0,1000):
  X_train, X_test, y_train, y_test =\
    train_test_split(X, y, 
                     test_size=0.3, 
                     random_state=i)
  stdsc = StandardScaler()
  X_train_std = stdsc.fit_transform(X_train)
  X_test_std = stdsc.transform(X_test)
  logit = LogisticRegression()
  logit.fit(X_train_std, y_train)
  a.append(logit.score(X_train_std, y_train))
  b.append(i)
  if (maxm<logit.score(X_train_std, y_train)):
    maxm=logit.score(X_train_std, y_train)
  #print("Accuracy on test set: {:.3f}".format(logit.score(X_test_std, y_test)))
print("Logistic Regression::")
print ("Accuracy::",maxm*100)
y_pred_logit = logit.predict(X_test_std)
print("Predicted value: ",y_pred_logit)
plt.plot(b, a) 
   
plt.xlabel('X-axis') 

plt.ylabel('Accuracy') 
  
plt.title('Accuracy Graph') 
  
# function to show the plot 
plt.show()

for col in data.columns:
    plt.hist(data.loc[data['label'] == 0, col],label ="female")
    plt.hist(data.loc[data['label'] == 1, col],label="male")
    plt.title(col)
    plt.xlabel("Feature magnitude")
    plt.ylabel("Frequency")
    plt.legend(loc='upper right')
    plt.show()

from google.colab import drive
drive.mount('/content/drive')

#Train decision tree model


tree = DecisionTreeClassifier(random_state=0,max_depth=4)
tree.fit(X_train_std, y_train)

print("Decision Tree")
print("Accuracy on training set: {:.13f}".format(100*tree.score(X_train_std, y_train)))
print("Accuracy on test set: ",100*maxm)

y_pred_tree = tree.predict(X_test_std)
print("Predicted value: ",y_pred_tree)

precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred_tree, average='micro')
#print("Precision, Recall and fscore:",precision, recall, fscore,)

#Plot the graph for feature selection for decision tree and random forest
def plot_feature_importances_mydata(model):
    n_features = X_train_std.shape[1]
    plt.figure(figsize=(8,6))
    plt.barh(range(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), list(data))
    plt.title("Feature Selection")
    plt.xlabel("Variable importance")
    plt.ylabel("Independent Variable")
    plt.show()

plot_feature_importances_mydata(tree)

